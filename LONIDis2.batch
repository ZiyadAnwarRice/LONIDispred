#!/usr/bin/env bash

#SBATCH --job-name=Dispred
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=32
#SBATCH --partition=gpu2
#SBATCH --gres=gpu:4
#SBATCH --time=06:00:00
#SBATCH --account=loni_disorder01
#SBATCH --output=logs/%x-%j.log
#SBATCH --error=logs/%x-%j.log
#SBATCH --open-mode=append
#SBATCH --mail-user=mkabir3@uno.edu
#SBATCH --mail-type=BEGIN,END,FAIL

set -euo pipefail

cd /ddnB/work/$USER/LONIDispred
mkdir -p logs

echo "SLURM_JOB_ID=$SLURM_JOB_ID"
echo "SLURM_GPUS=${SLURM_GPUS:-none}"
echo "SLURM_GPUS_ON_NODE=${SLURM_GPUS_ON_NODE:-none}"
echo "CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES"

nvidia-smi -L
nvidia-smi

source /ddnB/work/$USER/LONIDispred/.venv/bin/activate
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

torchrun --standalone --nproc_per_node=4 \
  /ddnB/work/$USER/LONIDispred/LORADisPredTrain.py \
  --run_name "esm2_auc1_tokenmix_$(date +%Y%m%d-%H%M%S)" \
  --model_name esm2_t33_650M_UR50D \
  --max_sequence_length_train 1022 \
  --max_sequence_length_test 1022 \
  --numberofepochs 3 \
  --numberofbatch 2 \
  --learning_rate 8e-5 \
  --scheduler cosine \
  --warmup_ratio 0.12 \
  --weight_decay 0.05 \
  --gradient_clip 1.0 \
  --grad_accum_steps 4 \
  --rank 16 \
  --lora_alpha 32 \
  --lora_dropout 0.10 \
  --head crf \
  --head_hidden_dim 512 \
  --head_dropout 0.15 \
  --loss ce \
  --layer_pooling token_weighted \
  --amp bf16 \
  --data_backend local \
  --train_sequences "/work/$USER/LONIDispred/data/train_sequences.pkl" \
  --train_labels    "/work/$USER/LONIDispred/data/train_labels.pkl" \
  --testNox_sequences "/work/$USER/LONIDispred/data/Seq_CAID3NOX.pkl" \
  --testNox_labels    "/work/$USER/LONIDispred/data/target_CAID3NOX.pkl" \
  --testPDB_sequences "/work/$USER/LONIDispred/data/Seq_CAID3PDB.pkl" \
  --testPDB_labels    "/work/$USER/LONIDispred/data/target_CAID3PDB.pkl" \
  --val_sequences "/work/$USER/LONIDispred/data/testPDB_sequences.pkl" \
  --val_labels    "/work/$USER/LONIDispred/data/testPDB_labels.pkl" \
  --output_dir "/work/$USER/LONIDispred/outputs"
